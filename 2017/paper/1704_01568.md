## Best Practices for Applying Deep Learning to Novel Applications
   ===============================================================

- URL: https://arxiv.org/abs/1704.01568
- Abstract : This report is targeted to groups who are subject matter experts in their application but deep learning novices. It contains practical advice for those interested in testing the use of deep neural networks on applications that are novel for deep learning. We suggest making your project more manageable by dividing it into phases. For each phase this report contains numerous recommendations and insights to assist novice practitioners.

### 개요
실용적인 측면에서 Deep Learning을 적용하기 위한 과정가 다양한 프로젝트에 대한 설명을 하고 있음

### 논문내용
#### Phase 1: Getting prepared
  1.  프로젝트의 성공을 측정할 수 있는 지표를 만들어라
    - 어떤 측정 항목이 중요한가? 어느 것이 덜 중요한가? 를 고려
        - 메트릭의 목표를 정의
            - 당신의 목표는 인간 수준의 성과? 가능하다면 인간의 수행 능력을 정량적으로 파악하여 비교할 필요가 있음 (ex - 인간이 catcha를 인식하는 비율?)
            - softmax/cross entropy/log loss 등의 loss-function의 활용도 중요

  2. 네트워크를 쉽게 만들어라
    - 네트워크가 수행해야하는 작업이 쉬워 질수록 교육이 쉬워지고 성능이 향상
    - the easier the network’s job, the better it will perform
    - 가장 희소한 것은 인간의 시간! 쉽게 네트워크를 만들고 인간의 관여가 적게 돌아갈 수 있도록 처리

#### Phase 2: Preparing your data
1. 데이터를 얼마나 모을 것인지 정하라
    - 데이터는 많으면 많을 수록 좋음
    - 메트릭에 따라 데이터 수집 양을 결정해야
    - 트레이닝 데이터와 함께, test & validation 데이터도 필요
2. 훈련 데이터가 적을 경우 사용할 수 있는 방법

   | 방법              | 논문   |
   |:---------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
   | transfer learning | Weiss, Karl, Taghi M. Khoshgoftaar, and DingDing Wang. "A survey of transfer learning." Journal of Big Data 3.1 (2016): 1-40.                                                           |
   | domain adaptation | Patel, Vishal M., Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. "Visual domain adaptation: A survey of recent advances." IEEE signal processing magazine 32, no. 3 (2015): 53- 69. |
   |                   | Gabriela Csurka   “Domain Adaptation for Visual Applications: A Comprehensive Survey”, CoRR, arXiv:1702.05374, 2017.                                                                                                                                                                                        |

3. 데이터에 대해서 분석해라
    - Class별 통계 수치에 대해서 확인 필요
        - 클래스가 imbalance 하진 않은가?
    - 어떤 전처리 방법을 사용해야 하는가?
        - Data Normalization --> 정규화는 Training Dataset간의 유사성을 높임으로써 네트워크 작업빠르게 만듬
        - Dimension Reduction

#### Phase 3: 가장 유사한 Deep Learning Application 탐색
1. 기존 실행되었던 application중 가장 유사한 DL Architecture를 탐색
2. 발견되면 해당 DL Network의 코드를 다운로드!
    - 가능하면 모든 코드를 다운로드해서 사용해라
    - 나중에 baseline을 설정하는데 도움이 됨
3. 기존 다양한 방법은 논문내에 표로 정리했음
    - These include image classification/object recognition (convolutional networks), processing sequential data (RNN/LSTM/GRU) such as language processing, and complex decision making (deep reinforcement learning).  There are also a number of other applications that are common, such as image segmentation and super-resolution (fully convolutional networks) and similarity matching (Siamese networks).
    - https://scholar.google.com , https://arxiv.org 을 열심히 검색!

#### Phase 4: Create a simple baseline model  (간단한 기본 모델 만들기)
1. Always start simple, small, and easy. 간단하고 작고 쉬운 baseline model을 생성하라
2. 생각보다 작은 Architecture로 시작하라
3. 일반적인 목적함수를 사용해라
4. hyper parameter는 common-setting을 활용(일반적인 설정을 사용하라는 얘기인듯)
5. 데이터를 조금만 사용해서 학습 시켜라
6. 반복적인 업데이트를 통해 모델의 성능을 향상시켜라
7. 일단 하나의 프레임워크(pytorch, tensoflow 등)만 사용해라

#### Phase 5: Create visualization and debugging tools
1. 지속적으로 결과를 측정해라 , “code once, measure twice"
2. 산출물을 평가하고, 네크워크를 시각화하고, 내부 요소의 값을 측정하여 얻는 결과에 대해 이해해야함
3. 네트워크에 대한 일반적인 이해가 있어야함 -  general understanding of problems
    - Overfitting 과 Underfitting
    - Overfitting은 네트워크를 규모를 확대함
    - Underfitting은 교육 데이터 세트의 크기를 늘려볼 것
4. 최대한 많이 시각화하라
    - Tensorboard는 좋은 Tool
5. Training이나 Test 오류를 비교하고 인간 수준의 성능과 비교
6. 최악의 문제를 먼저 디버깅 - Training Data 문제인지, Network 구조 문제인지, Loss function 문제인지를 확인

#### Phase 6: Fine tune your model
1. 성능 향상을 위한 여러가지 튜닝을 실시
    - 아키텍처 설계, 깊이, width, 경로, weight 초기화, loss-function 등을 변경
    - learning rate range test
    - Leslie N. Smith. Cyclical learning rates for training neural networks. In Proceedings of the IEEE Winter Conference on Applied Computer Vision, 2017.
    - regularization methods, such as data augmentation, dropout, and weight decay
2. loss-function에 대해서 실험하라
    - 더 복잡한 손실함수 더 좋은 결과를 가지고 오는가?
    - loss-function과 metric간의 관계를보고 weighted components를 loss-function에 추가함
3. Phase 4에서 선택한 모델외에도 2~3번째로 유사한 모델에도 적용해서 결과를 비교해볼 것

Phase 7: End-to-end training, ensembles and other complexities
1. 시간과 여유가 있다면 End-to-end training와 앙상블 기법을 사용해라

### 내가 관심있는 논문 목록

| 주제                                                | 방법                                                     | 논문                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|:----------------------------------------------------|:---------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Automatic Machine Translation.                      | Stacked networks of large LSTM recurrent neural networks | [http://www.nlpr.ia.ac.cn/cip/ZongPublications/](http://www.nlpr.ia.ac.cn/cip/ZongPublications/2015/IEEE-Zhang-8-5.pdf)[](http://www.nlpr.ia.ac.cn/cip/ZongPublications/2015/IEEE-Zhang-8-5.pdf)[2015/IEEE-Zhang-8-5.pdf](http://www.nlpr.ia.ac.cn/cip/ZongPublications/2015/IEEE-Zhang-8-5.pdf)[https://arxiv.org/abs/1612.06897](https://arxiv.org/abs/1612.06897)[](https://arxiv.org/abs/1611.04558)[https://arxiv.org/abs/1611.04558](https://arxiv.org/abs/1611.04558) |
| Object Classification in Photographs.               | Residual CNNs, ResNeXt, Densenets                        | [http://papers.nips.cc/paper/5207-deep-neural-](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf)[](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf)[networks-for-object-detection.pdf](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf)                                                                                                                                         |
| Character Text Generation.                          | RNNs                                                     | [http://arxiv.org/pdf/1308.0850v5.pdf](http://arxiv.org/pdf/1308.0850v5.pdf)                                                                                                                                                                                                                                                                                                                                                                                                 |
| Modifying synthetic data into labeled training data | GAN                                                      | [https://arxiv.org/abs/1701.05524](https://arxiv.org/abs/1701.05524)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Time series analysis                                | Resnet + RNNs                                            | [https://arxiv.org/abs/1701.01887](https://arxiv.org/abs/1701.01887)[](https://arxiv.org/abs/1611.06455)[https://arxiv.org/abs/1611.06455](https://arxiv.org/abs/1611.06455)                                                                                                                                                                                                                                                                                                 |
| Visual question answering                           | Resnet                                                   | [https://arxiv.org/abs/1612.05386](https://arxiv.org/abs/1612.05386)                                                                                                                                                                                                                                                                                                                                                                                                         |
| Natural Language Processing (NLP)|      LSTM, GRU  | [https://arxiv.org/abs/1606.0673](https://arxiv.org/abs/1606.06737v2)|
